# configs/ppo_breakout.yaml
env_id: ALE/Breakout-v5
seed : 42

train:
  n_envs: 8              # Number of parallel actors (paper uses 8)
  n_steps: 128           # Horizon (T) - steps per rollout per env (paper: 128)
  total_steps: 600000  # Total environment steps (10M steps = 40M frames with skip=4)
  eval_interval: 200000  # Evaluate every 200k steps
  eval_episodes: 30      # Number of episodes per evaluation
  save_interval: 200000  # Save checkpoint every 200k steps

agent:
  # Core PPO hyperparameters (from paper Table 5)
  gamma: 0.99            # Discount factor
  gae_lambda: 0.95       # GAE parameter λ
  n_epochs: 3            # Number of optimization epochs per rollout (paper: 3, NOT 4!)
  batch_size: 256        # Minibatch size (32 × 8 = 256)
  
  # Clipping and regularization
  clip_range: 0.1        # Initial clipping parameter ε (will be annealed to 0)
  ent_coef: 0.01         # Entropy coefficient c2 (paper: 0.01)
  vf_coef: 1.0           # Value function coefficient c1 (paper: 1.0, NOT 0.5!)
  max_grad_norm: 0.5     # Gradient clipping (not in paper, but standard practice)
  
  # Learning rate
  lr: 2.5e-4             # Initial Adam learning rate (will be annealed to 0)
  
  # CRITICAL: Learning rate and clip range annealing
  # Paper uses α linearly annealed from 1 to 0 over training
  # lr_actual = lr × α,  clip_range_actual = clip_range × α
  anneal_lr: true        # Enable learning rate annealing
  anneal_clip: true      # Enable clip range annealing
  
  # Environment settings
  clip_rewards: true     # Clip rewards to {-1, 0, +1} for training
  full_action_space: false
  sticky_action_prob: null

# Early stopping (optional but recommended)
early_stopping:
  enabled: false         # Set to true to enable
  target_kl: 0.01       # Stop policy updates if mean KL divergence > threshold

# Notes:
# 1. The paper anneals BOTH learning rate and clip range linearly from initial value to 0
# 2. This is critical for good performance - without it, training is suboptimal
# 3. Value function coefficient is 1.0 in the paper, not 0.5
# 4. Using 3 epochs (not 4) matches the paper exactly
# 5. Expected performance: 300-400+ average reward on Breakout after 10M steps
